# crawld Performance Guide

This document provides comprehensive performance benchmarks, optimization techniques, and best practices for using crawld efficiently.

## üìã Table of Contents

- [Performance Overview](#performance-overview)
- [Benchmarks](#benchmarks)
- [Optimization Techniques](#optimization-techniques)
- [Performance Tuning](#performance-tuning)
- [Monitoring and Profiling](#monitoring-and-profiling)
- [Resource Usage](#resource-usage)
- [Best Practices](#best-practices)
- [Troubleshooting Performance Issues](#troubleshooting-performance-issues)

## üèÅ Performance Overview

crawld is designed for high-performance web crawling with the following characteristics:

### Key Performance Features

- **Concurrent Processing**: Configurable worker pool for parallel crawling
- **Connection Reuse**: HTTP connection pooling for reduced overhead
- **Memory Efficiency**: Streaming processing without storing full page content
- **Adaptive Rate Limiting**: Configurable request rates to prevent server overload
- **Early Termination**: Depth limits and domain filtering to avoid unnecessary work

### Performance Goals

- **Throughput**: 20-100+ URLs per second depending on site characteristics
- **Latency**: Sub-second response for individual URL processing
- **Memory**: <100MB for typical crawls of 10,000+ URLs
- **CPU**: Efficient multi-core utilization through goroutines

## üìä Benchmarks

### Test Environment

- **Hardware**: MacBook Pro M2, 16GB RAM
- **Network**: 100 Mbps connection
- **Go Version**: 1.21.0
- **crawld Version**: v1.0.0

### Small Sites (< 100 URLs)

| Concurrent Workers | URLs/Second | Memory Usage | CPU Usage |
|-------------------|-------------|---------------|-----------|
| 5                 | 45-55       | 15-20MB       | 25-35%    |
| 10                | 65-75       | 20-25MB       | 40-50%    |
| 20                | 80-90       | 25-30MB       | 60-70%    |
| 50                | 85-95       | 35-45MB       | 80-90%    |

### Medium Sites (100-1,000 URLs)

| Concurrent Workers | URLs/Second | Memory Usage | CPU Usage |
|-------------------|-------------|---------------|-----------|
| 10                | 35-45       | 25-35MB       | 30-40%    |
| 20                | 50-60       | 35-45MB       | 50-60%    |
| 30                | 60-70       | 45-55MB       | 70-80%    |
| 50                | 65-75       | 55-70MB       | 85-95%    |

### Large Sites (> 1,000 URLs)

| Concurrent Workers | URLs/Second | Memory Usage | CPU Usage |
|-------------------|-------------|---------------|-----------|
| 20                | 25-35       | 45-60MB       | 40-50%    |
| 30                | 35-45       | 60-80MB       | 60-70%    |
| 50                | 40-50       | 80-120MB      | 80-90%    |
| 100               | 45-55       | 120-200MB     | 95-100%   |

### Performance by Site Type

| Site Type        | Avg URLs/Sec | Notes                           |
|------------------|-------------|---------------------------------|
| Static Sites     | 80-100      | Fast response, small pages     |
| News Sites       | 40-60       | Medium complexity, more content |
| E-commerce       | 20-40       | Complex pages, heavy content    |
| Social Media     | 15-30       | Rate limiting, complex content  |
| Documentation    | 60-80       | Simple structure, fast response |

## ‚ö° Optimization Techniques

### 1. Worker Pool Sizing

#### Finding Optimal Worker Count

```bash
# Test different worker counts
for workers in 5 10 20 30 50; do
    echo "Testing with $workers workers:"
    time crawld --concurrent $workers --depth 3 https://example.com > /dev/null
done
```

#### Optimal Worker Count Formula

```
Optimal Workers = 2-4 √ó Number of CPU Cores
```

For I/O-bound operations (typical web crawling):
- Start with 2√ó CPU cores
- Increase gradually while monitoring performance
- Stop when throughput plateaus or memory usage becomes excessive

### 2. Memory Optimization

#### Streaming Processing

```go
// Good: Stream processing without storing full content
func (p *Parser) ExtractLinks(reader io.Reader) ([]string, error) {
    doc, err := goquery.NewDocumentFromReader(reader)
    // Process immediately, don't store content
}

// Avoid: Loading full content into memory
func (p *Parser) ExtractLinksFromString(content string) ([]string, error) {
    // Large content strings consume excessive memory
}
```

#### Memory Pool Usage

- HTTP connection pooling reduces allocation overhead
- Reuse buffers for URL processing
- Garbage collection tuning for high-throughput scenarios

### 3. Network Optimization

#### Connection Reuse

```bash
# Monitor connection reuse
crawld --verbose https://example.com 2>&1 | grep -i "connection"
```

#### Request Batching

- Process multiple URLs per worker efficiently
- Minimize context switching overhead
- Balance batch size with memory usage

### 4. Rate Limiting Strategy

#### Respectful Crawling

```bash
# Conservative approach for production
crawld --concurrent 10 --rate-limit 5 --depth 5 https://example.com

# Aggressive approach for testing (use with caution)
crawld --concurrent 50 --rate-limit 20 --depth 3 https://example.com
```

#### Adaptive Rate Limiting

- Start conservative and increase based on server response
- Monitor error rates and adjust accordingly
- Implement backoff on 429 (Too Many Requests) responses

## üîß Performance Tuning

### Command-Line Optimization

#### For Small Sites (< 100 URLs)
```bash
crawld --concurrent 20 --depth 3 https://small-site.com
```

#### For Medium Sites (100-1,000 URLs)
```bash
crawld --concurrent 30 --rate-limit 10 --depth 5 https://medium-site.com
```

#### For Large Sites (> 1,000 URLs)
```bash
crawld --concurrent 50 --rate-limit 15 --depth 5 https://large-site.com --verbose
```

#### Memory-Constrained Environments
```bash
crawld --concurrent 10 --rate-limit 5 --depth 3 https://example.com
```

#### High-Performance Scenarios
```bash
crawld --concurrent 100 --rate-limit 50 --depth 3 https://fast-server.com
```

### Environment-Specific Tuning

#### Docker Optimization

```bash
# Increase memory limit for large crawls
docker run --memory=1g --rm ghcr.io/aoshimash/crawld:latest \
  --concurrent 50 --depth 5 https://example.com

# CPU optimization
docker run --cpus="4.0" --rm ghcr.io/aoshimash/crawld:latest \
  --concurrent 40 https://example.com
```

#### CI/CD Optimization

```bash
# Conservative settings for CI environments
crawld --concurrent 5 --rate-limit 2 --depth 2 https://example.com
```

## üìà Monitoring and Profiling

### Built-in Performance Monitoring

#### Verbose Output Analysis

```bash
crawld --verbose https://example.com 2>&1 | grep -E "(rate|time|memory)"
```

#### Statistics Tracking

```bash
# Example output with performance metrics
crawld --verbose https://example.com
# 2024/01/01 12:00:00 INFO Starting crawl url=https://example.com depth=0 workers=10
# 2024/01/01 12:00:05 INFO Crawl completed total=150 successful=145 failed=5 time=5.2s
```

### Go Profiling

#### CPU Profiling

```bash
# Build with profiling support
go build -o crawld-prof -tags=profile ./cmd/crawld

# Run with CPU profiling
crawld-prof --concurrent 50 https://example.com
```

#### Memory Profiling

```bash
# Monitor memory usage during crawling
go tool pprof crawld crawld.mem
```

#### Benchmarking

```bash
# Run performance benchmarks
go test -bench=. -benchmem ./internal/crawler/
```

### External Monitoring

#### Resource Usage Monitoring

```bash
# Monitor resource usage during crawling
top -p $(pgrep crawld)

# Memory usage tracking
ps -o pid,rss,vsz -p $(pgrep crawld)

# Network monitoring
netstat -i
```

## üíæ Resource Usage

### Memory Usage Patterns

#### Baseline Memory
- **Minimal**: 10-15MB (small crawls, low concurrency)
- **Typical**: 30-50MB (medium crawls, moderate concurrency)
- **High**: 100-200MB (large crawls, high concurrency)

#### Memory Scaling Factors

1. **Worker Count**: ~1-2MB per worker
2. **URL Queue Size**: ~100 bytes per queued URL
3. **Result Storage**: ~150 bytes per discovered URL
4. **HTTP Connections**: ~50KB per active connection

#### Memory Formula

```
Total Memory ‚âà Base (15MB) + (Workers √ó 2MB) + (URLs √ó 250 bytes)
```

### CPU Usage Characteristics

#### CPU Scaling
- **Linear Scaling**: Up to number of CPU cores
- **Diminishing Returns**: Beyond 2-4√ó CPU cores
- **Optimal Range**: 50-80% CPU utilization

#### CPU Intensive Operations
1. **HTML Parsing**: 30-40% of CPU time
2. **URL Processing**: 20-30% of CPU time
3. **Network I/O**: 20-30% of CPU time
4. **Goroutine Management**: 10-20% of CPU time

### Network Usage

#### Bandwidth Considerations
- **Typical**: 1-5 Mbps for most crawls
- **High-throughput**: 10-50 Mbps for aggressive crawling
- **Connection Limit**: OS-dependent (typically 1000+ concurrent)

#### Network Optimization
- Connection pooling reduces overhead
- Keep-alive connections improve efficiency
- DNS caching reduces lookup time

## üéØ Best Practices

### Pre-Crawl Planning

1. **Site Analysis**
   ```bash
   # Test single page first
   curl -I https://example.com

   # Check robots.txt
   curl https://example.com/robots.txt
   ```

2. **Conservative Start**
   ```bash
   # Start with low concurrency
   crawld --concurrent 5 --depth 2 https://example.com
   ```

3. **Gradual Scaling**
   ```bash
   # Increase based on performance
   crawld --concurrent 10 --depth 3 https://example.com
   crawld --concurrent 20 --depth 5 https://example.com
   ```

### During Crawling

1. **Monitor Progress**
   ```bash
   # Use verbose mode for monitoring
   crawld --verbose --concurrent 20 https://example.com
   ```

2. **Resource Monitoring**
   ```bash
   # Monitor system resources
   htop
   iotop
   ```

3. **Graceful Termination**
   ```bash
   # Use Ctrl+C for graceful shutdown
   # crawld handles SIGINT properly
   ```

### Post-Crawl Analysis

1. **Performance Review**
   - Analyze throughput rates
   - Review error rates
   - Check resource usage peaks

2. **Optimization Planning**
   - Identify bottlenecks
   - Plan parameter adjustments
   - Consider infrastructure changes

## üêõ Troubleshooting Performance Issues

### Common Performance Problems

#### Slow Crawling

**Symptoms**: Low URLs/second, high response times

**Diagnosis**:
```bash
# Check network connectivity
ping example.com

# Test single request
curl -w "%{time_total}" https://example.com

# Check DNS resolution
nslookup example.com
```

**Solutions**:
- Increase concurrent workers
- Check network connectivity
- Verify target server performance
- Reduce rate limiting if appropriate

#### High Memory Usage

**Symptoms**: Excessive RAM consumption, potential OOM

**Diagnosis**:
```bash
# Monitor memory usage
ps aux | grep crawld

# Check for memory leaks
valgrind crawld https://example.com
```

**Solutions**:
- Reduce concurrent workers
- Limit crawl depth
- Implement stricter URL filtering
- Check for application memory leaks

#### CPU Bottlenecks

**Symptoms**: 100% CPU usage, slow processing

**Diagnosis**:
```bash
# Check CPU usage
top -p $(pgrep crawld)

# Profile CPU usage
go tool pprof crawld cpu.prof
```

**Solutions**:
- Optimize HTML parsing logic
- Reduce concurrent workers
- Use more efficient data structures
- Consider horizontal scaling

#### Network Issues

**Symptoms**: Connection timeouts, high error rates

**Diagnosis**:
```bash
# Check network statistics
netstat -s

# Monitor connections
ss -tuln | grep crawld
```

**Solutions**:
- Implement retry logic
- Increase connection timeouts
- Reduce concurrent connections
- Check firewall settings

### Performance Testing Methodology

#### Baseline Testing

```bash
# Establish baseline performance
time crawld --concurrent 10 --depth 2 https://example.com > baseline.txt
```

#### Load Scaling

```bash
# Test scaling characteristics
for workers in 5 10 20 30 50; do
    echo "Workers: $workers"
    time crawld --concurrent $workers --depth 3 https://example.com > /dev/null
done
```

#### Stress Testing

```bash
# Test maximum performance
crawld --concurrent 100 --rate-limit 100 --depth 5 https://fast-server.com
```

#### Endurance Testing

```bash
# Test long-running stability
timeout 1h crawld --concurrent 20 --depth 10 https://large-site.com
```

---

This performance guide provides the foundation for optimizing crawld for your specific use cases and infrastructure requirements.
